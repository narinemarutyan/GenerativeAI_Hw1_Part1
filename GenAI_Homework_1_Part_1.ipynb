{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj-l6TqdN7ZP"
   },
   "source": [
    "# GenAI Homework 1 Part 1\n",
    "\n",
    "#### Make sure that you read your API Key from a file and submit the api key file with your homework. Only TA and I will have access to your api key to check your homework. I will ask to delete api keys shared with us after checking the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6fd4u4GNnud"
   },
   "source": [
    "**Task 1:** Create an AI assistant that will answer \"What's the temperature outside now\" or \"What's the temperature in Tokio now\" type of questions (location can be any big city). Ask for the location of the user if the location is not in the question. Use OpenAI APIs, [openweathermap](http://api.openweathermap.org/data/2.5/weather) API(it is free), and function calling.\n",
    "\n",
    "* Step 1: Use OpenAI Chat Completions API to get the location of the user if it is not given. If it is given, use function calling for getting weather api parameter(s).\n",
    "* Step 2: Call weather api for the given location.\n",
    "* Step 3: Call Chat Completions API again for processing the response of weather api. Make it to provide short answer like this: \"The temperature in Yerevan is -1.91 degrees Celsius\".\n",
    "* Step 4: Call Chat Completions API again to translate the output of Step 3 into Armenian.\n",
    "* Step 5: Use OpenAI Text to Speech API to create an audio version (mp3) of the output of Step 4.\n",
    "* Step 6: Use one of OpenAIs APIs to create an image based on the output of Step 3 (text to image).\n",
    "* Step 7: Use one of OpenAIs APIs to extract text (if any) from the output of Step 6\n",
    "* Step 8: Create a Chainlit app that will answer the questions mentioned at the beginning of the task and will output the outputs of Steps 3, 4, 5, 6, and 7.\n",
    "\n",
    "Useful links:\n",
    "* [Chainlit App Creation](https://docs.chainlit.io/get-started/pure-python])\n",
    "* [Text, Image, Audio and Video response with Chainlit](https://docs.chainlit.io/api-reference/elements/text)\n",
    "\n",
    "**Check a Chainlit app example at the end of this notebook.**\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scipy\n",
    "!pip install tenacity\n",
    "!pip install tiktoken\n",
    "!pip install termcolor\n",
    "!pip install openai\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:27:56.000951Z",
     "start_time": "2024-02-11T18:27:55.734589Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import openai\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "openai.api_key = \"openai-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
    "    }\n",
    "    json_data = {\"model\": model, \"messages\": messages}\n",
    "    if tools is not None:\n",
    "        json_data.update({\"tools\": tools})\n",
    "    if tool_choice is not None:\n",
    "        json_data.update({\"tool_choice\": tool_choice})\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_data,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Use OpenAI Chat Completions API to get the location of the user if it is not given. If it is given, use function calling for getting weather api parameter(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\", \"format\"],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'Sure, I can help you with that. Could you please provide me with the location?'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, tools=tools\n",
    ")\n",
    "print(chat_response)\n",
    "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': None,\n",
       " 'tool_calls': [{'id': 'call_yT2o2x7uFmyG8I6aplZS8X4E',\n",
       "   'type': 'function',\n",
       "   'function': {'name': 'get_current_weather',\n",
       "    'arguments': '{\\n  \"location\": \"Yerevan, Armenia\"\\n}'}}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"I'm in Yerevan, Armenia.\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, tools=tools\n",
    ")\n",
    "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = assistant_message['tool_calls'][0]['function']['arguments'].split('\"')[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Call weather api for the given location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coord': {'lon': 44.5136, 'lat': 40.1811}, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01n'}], 'base': 'stations', 'main': {'temp': 278.24, 'feels_like': 277.15, 'temp_min': 278.24, 'temp_max': 278.24, 'pressure': 1025, 'humidity': 56}, 'visibility': 10000, 'wind': {'speed': 1.54, 'deg': 70}, 'clouds': {'all': 10}, 'dt': 1707676153, 'sys': {'type': 1, 'id': 8851, 'country': 'AM', 'sunrise': 1707624072, 'sunset': 1707661878}, 'timezone': 14400, 'id': 616052, 'name': 'Yerevan', 'cod': 200}\n"
     ]
    }
   ],
   "source": [
    "url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "params = {\n",
    "    \"q\": loc,\n",
    "    \"APPID\": \"3128f76a7551a274746e884fd29a0e8f\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Call Chat Completions API again for processing the response of weather api. Make it to provide short answer like this: \"The temperature in Yerevan is -1.91 degrees Celsius\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The temperature in Yerevan, based on the given information, is 278.24 degrees Celsius.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=GPT_MODEL,\n",
    "  response_format={\"type\": \"text\"},\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are weather assistant\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{response.text} based on this information what the temerature in celcius in the given location. Provide short answer.\"}\n",
    "  ]\n",
    ")\n",
    "processed_response = response.choices[0].message.content\n",
    "processed_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Call Chat Completions API again to translate the output of Step 3 into Armenian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Այս տեքստը թարմացած է անգլերենից արեգելերենում:'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=GPT_MODEL,\n",
    "  response_format={\"type\": \"text\"},\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are an 'English to Armenian' translator.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{processed_response} translate this text to Armenian.\"}\n",
    "  ]\n",
    ")\n",
    "armenian_response = response.choices[0].message.content\n",
    "armenian_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Use OpenAI Text to Speech API to create an audio version (mp3) of the output of Step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b3/4741q4d54n7gygnt78sksqyw0000gn/T/ipykernel_4151/1551155152.py:6: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(\"weather_in_armenian.mp3\")\n"
     ]
    }
   ],
   "source": [
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1-hd\",\n",
    "  voice=\"alloy\",\n",
    "  input=armenian_response)\n",
    "\n",
    "response.stream_to_file(\"weather_in_armenian.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Use one of OpenAIs APIs to create an image based on the output of Step 3 (text to image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-ZzOlacmiAFcP64tkl2KTUBHK/user-PSAgu3eg7o5g8o8185XBqaYJ/img-qz8j5O2hC4JhZkd2aK3diSbI.png?st=2024-02-11T17%3A29%3A33Z&se=2024-02-11T19%3A29%3A33Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-02-11T17%3A21%3A20Z&ske=2024-02-12T17%3A21%3A20Z&sks=b&skv=2021-08-06&sig=NCSN%2BBhNFpbkMHDLvUv1eI%2BOi5ROxQMra8eHVONxAko%3D'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.images.generate(\n",
    "  model=\"dall-e-2\",\n",
    "  prompt=processed_response,\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"hd\",\n",
    "  n=1,\n",
    ")\n",
    "\n",
    "image_url = response.data[0].url\n",
    "image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Use one of OpenAIs APIs to extract text (if any) from the output of Step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This image depicts a close-up view of a split-flap display, typically found in train stations or airports to display arrival and departure information. The background is yellow, and we can see several flaps with white characters on a black background. Each flap appears to show part of a word, with visible fragments such as \"AG,\" \"AURA,\" \"ARNA,\" and \"ZOO.\" Below the alphabetical information are numbers that likely correspond to times or other numerical data. Split-flap displays are mechanically operated, and they make a characteristic sound as they flip through the characters to update the displayed information. These displays have become less common in recent years, being replaced by digital screens, but they are still appreciated for their nostalgic appeal and the kinetic aspect of their design.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4-vision-preview\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": image_url,\n",
    "            \"detail\": \"high\"\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "extracted_text = response.choices[0].message.content\n",
    "extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Create a Chainlit app that will answer the questions mentioned at the beginning of the task and will output the outputs of Steps 3, 4, 5, 6, and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
